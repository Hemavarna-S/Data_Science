{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrEEB9GSh6X42Ttsh4fH7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemavarna-S/Data_Science/blob/main/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqSYUPjaAfiD",
        "outputId": "83658f76-c7f9-445b-a8e5-082871ca2802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs"
      ],
      "metadata": {
        "id": "EHlMbQftAlqh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "YdrNAip1A2oD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \"http://books.toscrape.com/catalogue/category/books/travel_2/index.html\"\n",
        "HEADERS ={\n",
        "    \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64;x64)\"\n",
        "                  \"AppleWebKit/537.36 (KHTML, Like Gecko)\"\n",
        "                  \"Chrome/114.0.0.0 Safari/537/36\"\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "qW1O8nqPBIXT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def covert_rating(rating_text):\n",
        "  ratings = {\n",
        "      \"One\": 1,\n",
        "      \"Two\": 2,\n",
        "      \"Three\": 3,\n",
        "      \"Four\": 4,\n",
        "      \"Five\": 5\n",
        "  }\n",
        "  return ratings.get(rating_text, None)\n",
        ""
      ],
      "metadata": {
        "id": "qQWnsQ3_B0Kv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_page(page_num):\n",
        "  url = BASE_URL.format(page_num)\n",
        "  print(f\"Fetching:{url}\")\n",
        "  try:\n",
        "    res = requests.get(url)\n",
        "    res.raise_for_status()\n",
        "    soup = bs(res.text,'html.parser')\n",
        "    return soup\n",
        "  except requests.expections.RequestException as e:\n",
        "   print(f\"Failed to fetch {url}:{e}\")\n",
        "  return None\n"
      ],
      "metadata": {
        "id": "boZwdFXqDKXk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_book(soup):\n",
        "  books[]\n",
        "  articles = soup.findall('article',class_='product_pod')\n",
        "  for article in articles:\n",
        "  try:\n",
        "    title = article.find('h3')\n",
        "    price = article.find('p',class_='price_color').text[1:]\n",
        "    availability = article.find('p',class_='instock availability').text.strip()\n",
        "    rating = covert_rating(article.find('p',class_='star-rating')['class'][1])\n",
        "    books.append({\n",
        "        'Title':title,\n",
        "        'Price':price,\n",
        "        'Availability':availability,\n",
        "        'Rating':rating\n",
        "\n",
        "    })\n",
        "  except Exception as e:\n",
        "      print(f\"Failed to extract book data:{e}\")\n",
        "  return books"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "BIBGPTJMEdFk",
        "outputId": "5f597180-48f2-4742-e1ae-9774b223fc15"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-22-4150123336.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-22-4150123336.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    books[]\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_books(max_pages=5):\n",
        "  all_books = []\n",
        "  for page_num in range(1,max_pages+1):\n",
        "    soup = fetch_page(page_num)\n",
        "    if soup:\n",
        "      books = extract_book(soup)\n",
        "      all_books.extend(books)\n",
        "      time.sleep(random.uniform(1.5,3))\n",
        "    else:\n",
        "      print(f\"Failed to extract the book data:{e}\")\n",
        "      break\n",
        "    return books"
      ],
      "metadata": {
        "id": "Z1QV7-iVHtcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  max_pages = 5\n",
        "  books_data = scrape_books(max_pages)\n",
        "  df = pd.DataFrame(books)\n",
        "  print(df.head)\n",
        "  df.to_csv('books.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "G0TkZGvbIdrC",
        "outputId": "ff182f9b-2014-43b8-ce66-a8404fcc60a3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching:http://books.toscrape.com/catalogue/category/books/travel_2/index.html\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_book' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-4179168268.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmax_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mbooks_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_books\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-2471888657.py\u001b[0m in \u001b[0;36mscrape_books\u001b[0;34m(max_pages)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mbooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mall_books\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_book' is not defined"
          ]
        }
      ]
    }
  ]
}